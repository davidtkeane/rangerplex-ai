# RangerPlex Bug Fixes - November 25, 2025

## Issue 1: LM Studio "Invalid Model ID" Error

### Symptom
When selecting an LM Studio model (e.g., `openai/gpt-oss-20b`), the error "invalid model ID" appeared even though the Settings showed a green checkmark for the connection.

### Root Cause
The model routing logic in `ChatInterface.tsx` checked for "gpt" in the model name **before** checking if it was an LM Studio model. Since LM Studio models like `openai/gpt-oss-20b` contain "gpt", they were incorrectly routed to the OpenAI API instead of LM Studio.

**Problematic code order:**
```typescript
} else if ((modelToUse.includes('gpt') || modelToUse.includes('o1')) && !isPetChat) {
    // OpenAI - this caught LM Studio models with "gpt" in name!
} else if (settings.availableModels.lmstudio.includes(modelToUse)) {
    // LM Studio - never reached for gpt-named models
}
```

### Fix
Moved the LM Studio check **before** the OpenAI check in `components/ChatInterface.tsx` (lines 2857-2864):

```typescript
} else if ((modelToUse === ModelType.LMSTUDIO || settings.availableModels.lmstudio.includes(modelToUse)) && !isPetChat) {
    // LM Studio - check BEFORE OpenAI since LM Studio models might contain "gpt" in their names
    const actualModelId = modelToUse === ModelType.LMSTUDIO ? settings.lmstudioModelId : modelToUse;
    const res = await streamLMStudioResponse(...);
} else if ((modelToUse.includes('gpt') || modelToUse.includes('o1')) && !isPetChat) {
    // OpenAI
}
```

---

## Issue 2: better-sqlite3 Node.js Version Mismatch

### Symptom
```
Error: The module '.../better_sqlite3.node' was compiled against a different
Node.js version using NODE_MODULE_VERSION 141. This version of Node.js requires
NODE_MODULE_VERSION 127.
```

### Root Cause
The `better-sqlite3` native module was compiled for a different Node.js version than the one currently running.

### Fix
Reinstall the module to recompile for the current Node.js version:

```bash
rm -rf node_modules/better-sqlite3
npm install better-sqlite3
```

---

## Issue 3: Port 3010 Already in Use (EADDRINUSE)

### Symptom
```
Error: listen EADDRINUSE: address already in use :::3010
```

### Root Cause
A previous instance of the proxy server was still running on port 3010.

### Fix
Kill the process using the port before starting:

```bash
lsof -ti:3010 | xargs kill -9
npm start
```

---

## Issue 4: Ollama "Not Found" Error

### Symptom
After fixing LM Studio, Ollama models returned "Ollama API Error: Not Found".

### Root Cause
The default `ollamaModelId` in settings was `llama3`, but the actual installed Ollama models were `deepseek-r1:70b` and `Qwen2.5:latest`.

### Fix
1. Added a **Refresh** button to the Ollama settings to fetch available models
2. Changed the Model ID field from a text input to a dropdown selector
3. Auto-selects the first available model if the current one doesn't exist

**Changes in `components/SettingsModal.tsx`:**

Added `fetchOllamaModelsOnly()` function (lines 163-185):
```typescript
const fetchOllamaModelsOnly = async () => {
    const ollama = await fetchOllamaModels(localSettings.ollamaBaseUrl);
    if (ollama.length > 0) {
        setLocalSettings(prev => ({
            ...prev,
            availableModels: { ...prev.availableModels, ollama: ollama }
        }));
        // Auto-set the first model if current one isn't in the list
        if (!ollama.includes(localSettings.ollamaModelId)) {
            setLocalSettings(prev => ({ ...prev, ollamaModelId: ollama[0] }));
        }
    }
};
```

Added dropdown with Refresh button (lines 866-886):
```tsx
<select value={localSettings.ollamaModelId} onChange={...}>
    {localSettings.availableModels.ollama.map(m => (
        <option key={m} value={m}>{m}</option>
    ))}
</select>
<button onClick={fetchOllamaModelsOnly}>
    <i className="fa-solid fa-sync"></i> Refresh
</button>
```

---

## Issue 5: No Loading Effect for LM Studio Model Changes

### Symptom
Ollama models showed a nice loading animation when switching, but LM Studio models did not.

### Original Request
Extend the loading effect to LM Studio models.

### Extended Request
Show the loading effect for **all** model changes, not just local models.

### Fix
Updated `handleModelChange()` in `components/ChatInterface.tsx` (lines 118-122):

**Before:**
```typescript
// Only triggered for Ollama models
if (settings.availableModels.ollama.includes(model) && settings.ollamaLoadingEffect !== 'none') {
    setIsModelLoading(true);
    setTimeout(() => setIsModelLoading(false), 3000);
}
```

**After:**
```typescript
// Trigger loading effect for any model change
if (settings.ollamaLoadingEffect !== 'none') {
    setIsModelLoading(true);
    setTimeout(() => setIsModelLoading(false), 2000); // 2s display time
}
```

---

## Summary of Files Changed

| File | Changes |
|------|---------|
| `components/ChatInterface.tsx` | Fixed model routing order; extended loading effect to all models |
| `components/SettingsModal.tsx` | Added Ollama model refresh button and dropdown selector |

---

## Testing Checklist

- [x] LM Studio models with "gpt" in name route correctly
- [x] Ollama models can be refreshed and selected from dropdown
- [x] Loading effect shows for all model changes
- [x] Proxy server starts without port conflicts
- [x] better-sqlite3 compiles for correct Node.js version
